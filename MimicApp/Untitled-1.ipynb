{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the objective: use a LSTM model for prediction of sepsis shock using physionet2019 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import time\n",
    "# import custom libraries\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\DATA\\\\Tasks\\\\lib\\\\hk\")\n",
    "import hk_psql\n",
    "import dash_bootstrap_components\n",
    "\n",
    "import plotly.express as px  # (version 4.7.0 or higher)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "ADD_DATA = \"C:\\\\DATA\\\\data\\\\raw\\\\physionet_challenge_2019\\\\\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## putting al .psv files into one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(ADD_DATA+'training_setA') ]\n",
    "li=[]\n",
    "for i,f in enumerate(files[:10000]):\n",
    "    df = pd.read_csv(ADD_DATA+'training_setA\\\\'+f, delimiter='|')\n",
    "    df['id'] = i\n",
    "    li.append(df)\n",
    "df_A = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_A.to_csv(ADD_DATA+'df_AA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(ADD_DATA+'training_setB') ]\n",
    "li=[]\n",
    "for i,f in enumerate(files[:10000]):\n",
    "    df = pd.read_csv(ADD_DATA+'training_setB\\\\'+f, delimiter='|')\n",
    "    df['id'] = i\n",
    "    li.append(df)\n",
    "\n",
    "df_B = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_B.to_csv(ADD_DATA+'df_BB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2',\n",
       "       'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN',\n",
       "       'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
       "       'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
       "       'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
       "       'Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2',\n",
       "       'HospAdmTime', 'ICULOS', 'SepsisLabel', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(ADD_DATA+'df_AA.csv')\n",
    "df_test = pd.read_csv(ADD_DATA+'df_BB.csv')\n",
    "df_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a link for imputation tutorial\n",
    "# https://towardsdatascience.com/how-to-fill-missing-data-with-pandas-8cb875362a0d\n",
    "\n",
    "cols = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2',\n",
    "       'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN',\n",
    "       'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
    "       'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
    "       'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
    "       'Fibrinogen', 'Platelets']\n",
    "\n",
    "\n",
    "df_train[cols] = df_train.groupby('id')[cols].ffill().bfill()\n",
    "df_test[cols] = df_train.groupby('id')[cols].ffill().bfill()\n",
    "\n",
    "df_train[cols] = df_train[cols].fillna( df_train[cols].mean() )\n",
    "df_train[cols] = df_train[cols].fillna( 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 34)\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "# read csv and put data in a list of tensors (each patient)\n",
    "\n",
    "\n",
    "train_data={'X':[], 'y':[]}\n",
    "for id in df_train['id'].unique():\n",
    "    train_data['X'].append(df_train[df_train['id']==id][cols].values)\n",
    "    train_data['y'].append(df_train[df_train['id']==id]['SepsisLabel'].values)\n",
    "\n",
    "print( train_data['X'][0].shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 34)\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "# read csv and put data in a list of tensors (each patient)\n",
    "\n",
    "\n",
    "test_data={'X':[], 'y':[]}\n",
    "for id in df_test['id'].unique():\n",
    "    test_data['X'].append(df_test[df_test['id']==id][cols].values)\n",
    "    test_data['y'].append(df_test[df_test['id']==id]['SepsisLabel'].values)\n",
    "\n",
    "print( test_data['X'][0].shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 34) (54,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['X'][0].shape, train_data['y'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# LSTM settings\n",
    "DEBUG=True\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = train_data['X'][0].shape[-1]\n",
    "# sequence_length = 28 # it is variable in our example\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataLoader object\n",
    "\n",
    "class MinimalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = list(map(lambda a: torch.tensor(a).float(), X))\n",
    "        self.y = list(map(lambda a: torch.tensor(a).long(), y))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        sample = {\"X\": X, \"y\": y}\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # for padding\n",
    "    # https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n",
    "    #X = [i[0] for i in batch]\n",
    "    #y = [i[1] for i in batch]\n",
    "\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "train_dataset = MinimalDataset(train_data['X'], train_data['y'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = MinimalDataset(test_data['X'], test_data['y'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "<class 'list'>\n",
      "8\n",
      "torch.Size([54, 34])\n",
      "torch.Size([54])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'term' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22680\\3512696081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mterm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'term' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "#print(train_dataset[0])\n",
    "\n",
    "#print(train_dataset[0])\n",
    "for i, (X,y) in enumerate(train_loader):\n",
    "    print(type(X))\n",
    "    print(len(X))\n",
    "    print(X[0].shape)\n",
    "    print(y[0].shape)\n",
    "    term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    input_size - will be 1 in this example since we have only 1 predictor (a sequence of previous values)\n",
    "    hidden_size - Can be chosen to dictate how much hidden \"long term memory\" the network will have\n",
    "    output_size - This will be equal to the prediciton_periods input to get_x_y_pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size=2):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden=None\n",
    "    def forward(self, x, x_lens):\n",
    "        \n",
    "        # initialize the hidden state.\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "\n",
    "        out: (batch, seq_len, hidden_size)\n",
    "        h_n, c_n: (num_layers * num_directions, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        states = (self.h_n, self.c_n)\n",
    "\n",
    "        if DEBUG: print('input.shape = ',x.shape, x.get_device())\n",
    "        if DEBUG: print('states[0].shape = ', states[0].shape, states[0].get_device())\n",
    "\n",
    "        # if variable length: do padding\n",
    "\n",
    "        #x = pad_sequence(x, batch_first=True)\n",
    "        #x_lens = list(map(len, x))\n",
    "        #x = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        x_padded_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        x_padded_packed, (self.h_n, self.c_n) = self.lstm(x_padded_packed, states)\n",
    "        output_padded, output_lengths = pad_packed_sequence(x_padded_packed, batch_first=True)\n",
    "        if DEBUG: print('output_lengths',output_lengths)\n",
    "        if DEBUG: print('lstm_out.shape = ',output_padded.shape)\n",
    "        #if DEBUG: print('lstm_out.view(len(x), -1).shape = ',lstm_out.view(len(x), -1).shape)\n",
    "\n",
    "        # if variable length: unpadd\n",
    "        #lstm_out = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "\n",
    "        \n",
    "        out = self.linear(output_padded)\n",
    "        if DEBUG: print('fc_out.shape = ',out.shape)\n",
    "\n",
    "        #predictions = nn.functional.log_softmax(out)\n",
    "        #predictions = torch.swapaxes(predictions, 1, 2)\n",
    "        #predictions = predictions.transpose(1,2)\n",
    "        return out\n",
    "\n",
    "    def init_states(self, batch_size, device='cpu'):\n",
    "        '''\n",
    "        Initiate hidden states.\n",
    "        '''\n",
    "        # Shape for hidden state and cell state: num_layers * num_directions, batch, hidden_size\n",
    "        \n",
    "        #h_0 = torch.randn(1, self.batch_size, self.hidden_dim)\n",
    "        #c_0 = torch.randn(1, self.batch_size, self.hidden_dim)\n",
    "\n",
    "        # The Variable API is now semi-deprecated, so we use nn.Parameter instead.\n",
    "        # Note: For Variable API requires_grad=False by default;\n",
    "        # For Parameter API requires_grad=True by default.\n",
    "        \n",
    "        #h_0 = nn.Parameter(h_0, requires_grad=True)\n",
    "        #c_0 = nn.Parameter(c_0, requires_grad=True)\n",
    "\n",
    "        \"\"\"initialize the hidden and cell states\"\"\"\n",
    "        self.h_n = torch.zeros(1, batch_size, self.hidden_size).float().to(device)\n",
    "        self.c_n = torch.zeros(1, batch_size, self.hidden_size).float().to(device)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape =  torch.Size([8, 24, 34]) -1\n",
      "states[0].shape =  torch.Size([1, 8, 5]) -1\n",
      "lstm_out.shape =  torch.Size([8, 24, 5])\n",
      "fc_out.shape =  torch.Size([8, 24, 2])\n",
      "----------------------------------------------------------------------------------------\n",
      "      Layer (type)                         Output Shape         Param #     Tr. Param #\n",
      "========================================================================================\n",
      "            LSTM-1     [8, 24, 5], [1, 8, 5], [1, 8, 5]             820             820\n",
      "          Linear-2                           [8, 24, 2]              12              12\n",
      "========================================================================================\n",
      "Total params: 832\n",
      "Trainable params: 832\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "========================= Hierarchical Summary =========================\n",
      "\n",
      "myLSTM(\n",
      "  (lstm): LSTM(34, 5, batch_first=True), 820 params\n",
      "  (linear): Linear(in_features=5, out_features=2, bias=True), 12 params\n",
      "  (sigmoid): Sigmoid(), 0 params\n",
      "), 832 params\n",
      "\n",
      "\n",
      "========================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hokarami\\AppData\\Local\\Temp\\ipykernel_22680\\3228273169.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = nn.functional.log_softmax(out)\n"
     ]
    }
   ],
   "source": [
    "# https://pypi.org/project/pytorch-model-summary/\n",
    "\n",
    "\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "model = myLSTM(input_size, hidden_size)\n",
    "model.init_states(batch_size)\n",
    "\n",
    "\n",
    "print(summary(model, torch.zeros((batch_size, 24, input_size)), show_input=False, show_hierarchical=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #model.init_states(batch_size, device=device)\n",
    "\n",
    "        for i, (X_pad, y_pad, X_lens, y_lens) in enumerate(train_loader):\n",
    "\n",
    "            model.init_states(batch_size, device=device)\n",
    "            \n",
    "            \n",
    "            X_padded = X_pad.to(device)\n",
    "            y_padded = y_pad.to(device)\n",
    "\n",
    "            print('X_padded.shape = ', X_padded.shape)\n",
    "            print('y_padded.shape = ', y_padded.shape)\n",
    "            print('len(lens) = ', (X_lens))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_padded, x_lens=X_lens)\n",
    "            print('y_pred.shape = ', y_pred.shape)\n",
    "\n",
    "            y_pred = torch.swapaxes(y_pred, 1, 2)\n",
    "            loss = criterion(y_pred, y_padded)\n",
    "            print('loss',loss)\n",
    "            # y_pred [N, C, d1,d2], y_target = [N,d1,d2]\n",
    "\n",
    "            # Backward and optimize\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \"\"\"Return the accuracy on the input data set\"\"\"\n",
    "    # Set the network to evalution mode\n",
    "    model.eval()\n",
    "    model.init_states(batch_size)\n",
    "    \n",
    "    # Total number of correctly classified samples\n",
    "    n_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X_padded = pad_sequence(X, batch_first=True)\n",
    "            y_padded = pad_sequence(y, batch_first=True)\n",
    "\n",
    "            y_pred = model(X_padded)\n",
    "           \n",
    "            # Compute the accuracy on the mini-batch.\n",
    "            # Update n_correct which count the total\n",
    "            # number of correctly classified samples.\n",
    "            #\n",
    "            # (3 lines of code)\n",
    "            # YOUR CODE HERE\n",
    "            max_val,arg_val = torch.max(y_pred,axis=1)\n",
    "            n_corr_batch = torch.eq(y_padded, arg_val).sum().item()\n",
    "            n_correct +=n_corr_batch\n",
    "            \n",
    "    return float(n_correct)/float(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n",
      "X_padded.shape =  torch.Size([8, 54, 34])\n",
      "y_padded.shape =  torch.Size([8, 54])\n",
      "len(lens) =  [54, 23, 48, 29, 48, 17, 45, 40]\n",
      "input.shape =  torch.Size([8, 54, 34]) 0\n",
      "states[0].shape =  torch.Size([1, 8, 5]) 0\n",
      "output_lengths tensor([54, 23, 48, 29, 48, 17, 45, 40])\n",
      "lstm_out.shape =  torch.Size([8, 54, 5])\n",
      "fc_out.shape =  torch.Size([8, 54, 2])\n",
      "y_pred.shape =  torch.Size([8, 54, 2])\n",
      "loss tensor(0.7947, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "X_padded.shape =  torch.Size([8, 258, 34])\n",
      "y_padded.shape =  torch.Size([8, 258])\n",
      "len(lens) =  [258, 23, 34, 21, 39, 42, 15, 19]\n",
      "input.shape =  torch.Size([8, 258, 34]) 0\n",
      "states[0].shape =  torch.Size([1, 8, 5]) 0\n",
      "output_lengths tensor([258,  23,  34,  21,  39,  42,  15,  19])\n",
      "lstm_out.shape =  torch.Size([8, 258, 5])\n",
      "fc_out.shape =  torch.Size([8, 258, 2])\n",
      "y_pred.shape =  torch.Size([8, 258, 2])\n",
      "loss tensor(0.6851, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [20, 34]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22680\\3793029681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Perform training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22680\\3959610698.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\dashVis\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\dashVis\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [20, 34]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "model = myLSTM(input_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = []\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "beg = time.perf_counter()\n",
    "print(f\"Start training for {num_epochs} epochs\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Perform training\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    epochs.append(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    # Test on validation set\n",
    "    accuracy = test(model, test_loader)\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(f\"epoch {epoch} loss {train_loss:.03f} accuracy {accuracy:.03f}\")\n",
    "print(f\"Training took {time.perf_counter()-beg:.2f} seconds\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "pred = m(conv(data))\n",
    "output = loss(pred, target)\n",
    "print(pred.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(train_data):\n",
    "    train_data.sort(key=lambda data: len(data), reverse=True)\n",
    "    data_length = [len(data) for data in train_data]\n",
    "    train_data =  torch.nn.utils.rnn.pad_sequence(train_data, batch_first=True, padding_value=0)\n",
    "    return train_data, data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for data, length in train_dataloader:\n",
    "    print(data)\n",
    "    print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, length in train_dataloader:\n",
    "    data = rnn_utils.pack_padded_sequence(data, length, batch_first=True)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.LSTM(1, 5, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyData(train_x)\n",
    "train_dataloader = DataLoader(train_data, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "flag = 0\n",
    "for data, length in train_dataloader:\n",
    "    data = torch.nn.utils.rnn.pack_padded_sequence(data, length, batch_first=True)\n",
    "    output, hidden = net(data)\n",
    "    if flag == 0:\n",
    "        print(output)\n",
    "        flag = 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f29e1a05e7eec16e11fbddc27661320510c37e25360333d406961ee2486f09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dashVis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
